{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ako imamo diferencijabilnu funkciju koju hoćemo da minimizujemo možemo da koristimo metode gradijentnog spusta. Kako znamo da je gradijent vektor koji pokazuje pravac _lokalno_ najbržeg uspona, iterativno pravimo korake u smeru suprotnom gradijentu. Pošto je to samo lokalno najstrmiji pravac, ne pravimo velike korake (veličinu koraka kontrolišemo parametrom $\\alpha$, poznat i kao learning rate).\n",
    "\n",
    "Metod inercije (momentum) je unapređenje osnovnog gradijentnog spusta sa idejom da ako smo napravili više koraka u jednom smeru, verovatno treba tako i da nastavimo. Dakle, na trenutni korak utiču i prethodno napravljeni koraci. Ipak, nisu svi prethodni koraci jednako bitni - bitniji su nam skoriji koraci, nego oni od pre mnogo iteracija. Stoga koristimo parametar $\\beta \\in (0,1)$ da smanjimo važnost davno donesenih odluka.\n",
    "\n",
    "Nesterovljev metod je gotovo isti kao metod inercije, samo što prvo pravimo \"međukorak\" samo na osnovu inercije (jer bismo taj deo svakako napravili), pa tek onda u toj novoj tački računamo gradijent, i, naravno, idemo u smeru suprotnom od njega.\n",
    "\n",
    "Adam je jedan od najkorišćenijih algoritama u mašinskom učenju. Ideja je da na osnovu izgleda funkcije prilagodimo veličinu koraka.\n",
    "Pravimo ocenu prvog i drugog statističkog momenta (očekivanje gradijenta i kvadrata gradijenta). Što se tiče prvog momenta logika je ista kao kod inercije. Drugi momenat nam govori kakve su promene gradijenata. Ako su promene velike, to znači da treba da usporimo, jer verovatno \"skačemo\" oko nekog minimuma. Ako su promene male, znači da ne menjamo mnogo pravac i možemo da ubrzamo, odnosno povećamo korak. Dakle, veličina koraka treba da bude proporcionalna prvom, a obrnuto proporcionalna drugom momentu.\n",
    "Kao i obično, želimo da naše statističke ocene budu nepristrasne. Pokušajte da izvedete formulu za popravljanje ocena (u kodu m_hat i v_hat) koju smo koristili da bismo dobili nepristrasnost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x: np.ndarray):\n",
    "    \"\"\"Toy example function that is convex and has a global minimum at (0,0)\"\"\"\n",
    "    return 0.5*(x[0]**2 + 10*x[1]**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient(x: np.ndarray):\n",
    "    \"\"\"Gradient of f\"\"\"\n",
    "    return np.array([x[0], 10*x[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def alpha(i: int):\n",
    "    \"\"\"Calculates alpha (learning rate) based on iteration i\"\"\"\n",
    "    # return 0.01 // i\n",
    "    return 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gd(x0: np.ndarray, num_iters: int, alpha_fn: callable, eps: float):\n",
    "    \"\"\"Basic gradient descent\"\"\"\n",
    "    x = x0\n",
    "    for it in range(num_iters):\n",
    "        x_new = x - alpha_fn(it) * gradient(x)\n",
    "\n",
    "        if abs(f(x) - f(x_new)) < eps:\n",
    "            break\n",
    "\n",
    "        x = x_new\n",
    "\n",
    "    return x, it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([3.15039757e-02, 6.32595944e-19]), 413)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gd(x0=np.array([2,5]), num_iters=1000, alpha_fn=alpha, eps=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def momentum(x0: np.ndarray, num_iters: int, alpha_fn: callable, eps: float, beta: float):\n",
    "    \"\"\"Gradient descent with momentum - accumulate previous steps and account for them\"\"\"\n",
    "    x = x0\n",
    "    inertia = np.array([0,0])\n",
    "    for it in range(num_iters):\n",
    "        inertia = beta * inertia + alpha_fn(it) * gradient(x)\n",
    "        x_new = x - inertia\n",
    "\n",
    "        if abs(f(x) - f(x_new)) < eps:\n",
    "            break\n",
    "\n",
    "        x = x_new\n",
    "\n",
    "    return x, it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([2.18161734e-02, 2.25643742e-33]), 220)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "momentum(x0=np.array([2,5]), num_iters=1000, alpha_fn=alpha, eps=1e-5, beta=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nesterov(x0: np.ndarray, num_iters: int, alpha_fn: callable, eps: float, beta: float):\n",
    "    \"\"\"\n",
    "    Nesterov's gradient descent\n",
    "\n",
    "    Basic idea: since we are going to move for inertia vector either way, let's do\n",
    "    that first, and then calculate gradient at that point and, as usual, move\n",
    "    in the opposite direction of the gradient.\n",
    "    \"\"\"\n",
    "    x = x0\n",
    "    inertia = np.array([0,0])\n",
    "    for it in range(num_iters):\n",
    "        inertia = beta * inertia + alpha_fn(it) * gradient(x - inertia)\n",
    "        x_new = x - inertia\n",
    "\n",
    "        if abs(f(x) - f(x_new)) < eps:\n",
    "            break\n",
    "\n",
    "        x = x_new\n",
    "\n",
    "    return x, it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.00466766, 0.00275261]), 63)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nesterov(x0=np.array([2,5]), num_iters=1000, alpha_fn=alpha, eps=1e-5, beta=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adam(x0: np.ndarray, num_iters: int, alpha_fn: callable, eps: float, beta1: float, beta2: float, delta: float):\n",
    "    \"\"\"\n",
    "    Adam - Adaptive moments estimation\n",
    "\n",
    "    Idea: estimate first and second moment of the gradient and adapt your step based on them.\n",
    "    If the first moment is large - we made several steps in the same direction - we should increase step size.\n",
    "    If the second moment is large - we are making steps in different, opposite directions - we should decrease step size.\n",
    "    Analogous reasoning stands for when the first and second moments are small.\n",
    "    \"\"\"\n",
    "    x = x0\n",
    "    m = np.array([0, 0])\n",
    "    v = np.array([0, 0])\n",
    "    for it in range(1, num_iters + 1):\n",
    "        grad = gradient(x)\n",
    "        # exponential moving average (EMA)\n",
    "        m = beta1 * m + (1 - beta1) * grad\n",
    "        v = beta2 * v + (1 - beta2) * grad**2\n",
    "\n",
    "        # m and v are not unbiased estimates until the following step\n",
    "        m_hat = m / (1 - beta1 ** it)\n",
    "        v_hat = v / (1 - beta2 ** it)\n",
    "\n",
    "        # sqrt(v) to make m and v be on the same scale, not grad and grad**2\n",
    "        # +delta to avoid zero division\n",
    "        x_new = x - alpha_fn(it) * m_hat / (np.sqrt(v_hat) + delta)\n",
    "\n",
    "        if abs(f(x) - f(x_new)) < eps:\n",
    "            break\n",
    "\n",
    "        x = x_new\n",
    "\n",
    "    return x, it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1.36156731e-16, 1.15471411e-02]), 1377)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adam(x0=np.array([2,5]), num_iters=10000, alpha_fn=lambda x: 0.01, eps=1e-5, beta1=0.9, beta2=0.999, delta=1e-7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
